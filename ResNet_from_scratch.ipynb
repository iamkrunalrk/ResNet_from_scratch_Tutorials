{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ResNet18 From Scratch (With CIFAR-10)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BasicBlock for ResNet Implementation\n",
    "\n",
    "In this code, we define the `BasicBlock` class, which is the building block of a ResNet architecture. ResNet introduces residual connections, which allow the network to learn identity mappings more easily, and this class implements one such block. \n",
    "\n",
    "The block consists of:\n",
    "1. **Two Convolutional Layers**: Both convolution layers use 3x3 kernels, and batch normalization is applied after each convolution.\n",
    "2. **Residual Connection (Shortcut)**: The output from the convolutional layers is added to the input through the residual connection. This helps avoid vanishing gradients, enabling the network to train deeper models.\n",
    "3. **Stride and Padding**: The stride of the first convolution can be adjusted to reduce the spatial dimensions of the feature maps. If needed, the shortcut path will also be adjusted with a 1x1 convolution to match the dimensions.\n",
    "\n",
    "The following code implements the `BasicBlock` class, which will later be used to build the complete ResNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # First 3x3 convolutional layer with padding of 1 (to keep output size same as input)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)  # Batch Normalization after the first convolution\n",
    "        \n",
    "        # Second 3x3 convolutional layer with padding of 1\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)  # Batch Normalization after the second convolution\n",
    "        \n",
    "        # Shortcut (skip connection). Identity mapping by default\n",
    "        self.shortcut = nn.Identity()  # Default shortcut is an identity mapping (no change)\n",
    "        \n",
    "        # If the stride is not 1, the dimensions change, so we apply a 1x1 convolution\n",
    "        # to match the dimensions of the input and output\n",
    "        if stride != 1:            \n",
    "            self.shortcut = nn.Sequential(\n",
    "                            nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                            nn.BatchNorm2d(planes)\n",
    "                            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # First convolution followed by BatchNorm and ReLU activation\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Second convolution followed by BatchNorm\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Add the shortcut (skip connection) to the output\n",
    "        out += self.shortcut(x)  # This is the residual connection\n",
    "        \n",
    "        # Apply ReLU activation after the addition\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Architecture\n",
    "\n",
    "In this code, we define the `ResNet` class, which builds the entire residual network using the `BasicBlock` as its core component. This architecture is commonly referred to as ResNet-18 (or any ResNet model depending on the number of blocks). ResNet models use **residual blocks** to allow gradients to flow more easily through the network during training, enabling the training of very deep networks.\n",
    "\n",
    "Key Components of the `ResNet` Class:\n",
    "1. **Initial Convolution Layer**: The first convolutional layer uses 64 filters with a kernel size of 3x3. The stride is set to 1, meaning the spatial dimensions of the input image remain the same.\n",
    "\n",
    "2. **Residual Blocks**: We use the `_make_layer()` method to stack several residual blocks, where each block consists of two convolution layers with skip connections. The number of residual blocks in each layer is specified by the `num_blocks` argument.\n",
    "\n",
    "3. **Layer Configurations**: The network has 4 layers, where the first layer maintains the input size, and the following layers reduce the spatial size of the input using a stride of 2.\n",
    "\n",
    "4. **Global Average Pooling**: After passing through all the layers, we use global average pooling to reduce the spatial dimensions to 1x1, making the output size (batch size, 512).\n",
    "\n",
    "5. **Fully Connected Layer**: The output of the global average pooling is passed through a fully connected layer, which maps it to the desired number of output classes (e.g., 10 for CIFAR-10).\n",
    "\n",
    "The following code implements the complete ResNet architecture and the forward pass logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResNet model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        \n",
    "        # Initial convolution layer with 64 filters (3x3 kernels)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)  # Batch normalization after the first convolution\n",
    "        \n",
    "        # Define the layers by calling _make_layer() method for each block\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        \n",
    "        # Fully connected layer for final classification\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # Generate a list of strides: first block uses the given stride, rest use stride 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Add a block for each layer\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes  # Update input planes after each block\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        \n",
    "        # Initial convolution and batch normalization\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Pass through the layers\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        # Global average pooling to reduce dimensions\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        \n",
    "        # Flatten the output to feed into the fully connected layer\n",
    "        out = out.view(out.size(0), -1)  # Flatten the tensor\n",
    "        \n",
    "        # Final fully connected layer for classification\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Preparing the CIFAR-10 Dataset\n",
    "\n",
    "In this section, we will load the CIFAR-10 dataset, a commonly used dataset for training image classification models. CIFAR-10 consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The dataset is divided into a training set of 50,000 images and a test set of 10,000 images.\n",
    "\n",
    "We will use the `torchvision` library to load and preprocess the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation and DataLoader Setup for CIFAR-10\n",
    "\n",
    "We enhance the preprocessing pipeline for the CIFAR-10 dataset by adding data augmentation for the training set. Data augmentation helps prevent overfitting and makes the model more robust by introducing randomness during training.\n",
    "\n",
    "Key Augmentations:\n",
    "1. **Random Crop**: The `RandomCrop` transformation randomly crops a 32x32 section of the image, with a padding of 4 pixels. This introduces slight variations in the input data, enhancing the model's ability to learn from different image sections.\n",
    "2. **Random Horizontal Flip**: The `RandomHorizontalFlip` transformation randomly flips the image horizontally with a 50% probability. This makes the model more invariant to left-right orientations of objects in the images.\n",
    "3. **ToTensor**: Converts the images into PyTorch tensors, making them compatible with the neural network.\n",
    "\n",
    "For the test set, we only apply the `ToTensor()` transformation without any augmentation since we want the test set to represent the real-world data distribution.\n",
    "\n",
    "After applying these transformations, we load the CIFAR-10 dataset into PyTorch `DataLoader` objects, which handle batching, shuffling, and parallel data loading efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # Apply random cropping with padding\n",
    "    transforms.RandomHorizontalFlip(),      # Randomly flip the image horizontally\n",
    "    transforms.ToTensor(),                  # Convert image to tensor\n",
    "])\n",
    "\n",
    "test = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor (no augmentation for test data)\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and test datasets with the defined transformations\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test)\n",
    "\n",
    "# Create DataLoader for training data (batch size 128, shuffling, using 4 worker threads)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "# Create DataLoader for test data (batch size 100, no shuffling)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the ResNet-18 Model\n",
    "\n",
    "The `ResNet18()` function initializes and returns a ResNet-18 model. ResNet-18 is a specific configuration of the ResNet architecture with 18 layers, designed to work well on a variety of image classification tasks, such as CIFAR-10. \n",
    "\n",
    "1. **BasicBlock**:  \n",
    "   - The ResNet-18 architecture is based on the `BasicBlock` building block, which is a simple residual block containing two convolutional layers with batch normalization.\n",
    "   \n",
    "2. **Layer Configuration**:  \n",
    "   - The `[2, 2, 2, 2]` argument specifies the number of `BasicBlock` units in each of the four stages of the network:\n",
    "     - 2 blocks in the first stage\n",
    "     - 2 blocks in the second stage\n",
    "     - 2 blocks in the third stage\n",
    "     - 2 blocks in the fourth stage\n",
    "     \n",
    "   This structure allows the network to progressively learn more complex representations, with deeper layers capturing higher-level features.\n",
    "\n",
    "This function is called to create the ResNet-18 model instance used for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Training Configuration for ResNet-18 on CIFAR-10\n",
    "\n",
    "In this section, we prepare the training configuration for training the ResNet-18 model on the CIFAR-10 dataset. We will be using the following settings:\n",
    "\n",
    "1. **Device Configuration**: \n",
    "   - We specify that the model will run on a GPU (CUDA). \n",
    "   - `cudnn.benchmark = True` is set to optimize performance for the current input sizes, which is beneficial when the input sizes are fixed and known.\n",
    "\n",
    "2. **Model Setup**: \n",
    "   - We instantiate the `ResNet18` model and move it to the GPU.\n",
    "   - We use `torch.nn.DataParallel()` to enable parallelism if there are multiple GPUs available.\n",
    "\n",
    "3. **Learning Rate**: \n",
    "   - The learning rate is initially set to 0.1. This will be used by the optimizer to update the model's weights during training.\n",
    "\n",
    "4. **Loss Function**:\n",
    "   - We use **Cross Entropy Loss**, which is suitable for multi-class classification tasks like CIFAR-10.\n",
    "\n",
    "5. **Optimizer**:\n",
    "   - We use the **SGD (Stochastic Gradient Descent)** optimizer with:\n",
    "     - **Momentum** of 0.9 to help accelerate convergence by considering previous gradients.\n",
    "     - **Weight decay** (L2 regularization) of 0.0002 to reduce overfitting.\n",
    "\n",
    "The model will be saved to a file called `resnet18_cifar10.pth` after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cudnn.benchmark = True  # Optimizes for a fixed input size\n",
    "\n",
    "# Initialize the ResNet-18 model and move it to the GPU\n",
    "net = ResNet18()\n",
    "net = net.to(device)\n",
    "\n",
    "# Wrap the model in DataParallel for multi-GPU support (if available)\n",
    "net = torch.nn.DataParallel(net)\n",
    "\n",
    "# Define learning rate and file name for saving the model\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet18_cifar10.pth'\n",
    "\n",
    "# Define the loss function (Cross Entropy Loss for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer (SGD with momentum and weight decay)\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function for ResNet-18 on CIFAR-10\n",
    "\n",
    "The `train()` function defines the training loop for one epoch of the ResNet-18 model on the CIFAR-10 dataset. Here is a step-by-step breakdown of what happens:\n",
    "\n",
    "1. **Model Training Mode**:\n",
    "   - `net.train()` sets the model to training mode. This is necessary for layers like dropout or batch normalization, which behave differently during training vs. evaluation.\n",
    "\n",
    "2. **Batch-wise Processing**:\n",
    "   - We iterate over the `train_loader`, which loads batches of images and their corresponding labels.\n",
    "   - Each batch of images (`inputs`) and labels (`targets`) is moved to the GPU (`inputs = inputs.to(device)` and `targets = targets.to(device)`).\n",
    "\n",
    "3. **Forward Pass**:\n",
    "   - The model performs a forward pass to compute the outputs (predictions) for the current batch using `outputs = net(inputs)`.\n",
    "\n",
    "4. **Loss Calculation**:\n",
    "   - The loss is computed using the `criterion` (cross-entropy loss in this case) by comparing the model's predictions (`outputs`) with the true labels (`targets`).\n",
    "\n",
    "5. **Backpropagation and Optimization**:\n",
    "   - `optimizer.zero_grad()` clears the gradients from the previous step.\n",
    "   - `loss.backward()` computes the gradients of the loss with respect to the model's parameters.\n",
    "   - `optimizer.step()` updates the model's parameters using the computed gradients.\n",
    "\n",
    "6. **Tracking Metrics**:\n",
    "   - The training loss (`train_loss`) and the number of correct predictions (`correct`) are accumulated for each batch.\n",
    "   - Every 100th batch, the function prints the current training accuracy and loss for monitoring.\n",
    "\n",
    "7. **Epoch Summary**:\n",
    "   - After processing all batches in the epoch, the function prints the overall training accuracy and loss.\n",
    "\n",
    "This function allows you to track the model's progress during training, including the per-batch accuracy and loss, as well as the final accuracy and loss for the entire epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\n[ Train epoch: %d ]' % epoch)\n",
    "    net.train()  # Set the model to training mode\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Loop through the training data\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)  # Move inputs to the GPU\n",
    "        targets = targets.to(device)  # Move targets to the GPU\n",
    "        \n",
    "        outputs = net(inputs)  # Perform a forward pass through the model\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset the gradients from the previous step\n",
    "        loss = criterion(outputs, targets)  # Compute the loss\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        optimizer.step()  # Update the model weights\n",
    "        \n",
    "        train_loss += loss.item()  # Accumulate the training loss\n",
    "        \n",
    "        _, predicted = outputs.max(1)  # Get the predicted labels (index of max logit)\n",
    "        \n",
    "        total += targets.size(0)  # Update the total number of samples processed\n",
    "        correct += predicted.eq(targets).sum().item()  # Count correct predictions\n",
    "        \n",
    "        if batch_idx % 100 == 0:  # Print progress every 100 batches\n",
    "            print('\\nCurrent batch:', str(batch_idx))\n",
    "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
    "            print('Current benign train loss:', loss.item())\n",
    "\n",
    "    # Print the overall training results for the epoch\n",
    "    print('\\nTotal benign train accuracy:', 100. * correct / total)\n",
    "    print('Total benign train loss:', train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Function for ResNet-18 on CIFAR-10\n",
    "\n",
    "The `test()` function evaluates the ResNet-18 model on the CIFAR-10 test dataset. This function performs the following tasks:\n",
    "\n",
    "1. **Model Evaluation Mode**:\n",
    "   - `net.eval()` sets the model to evaluation mode. This ensures that layers like dropout or batch normalization behave correctly during testing (i.e., no dropout and the use of running statistics for batch normalization).\n",
    "\n",
    "2. **Batch-wise Processing**:\n",
    "   - The function iterates over the `test_loader`, which provides batches of test images and their corresponding labels.\n",
    "   - Each batch of images (`inputs`) and labels (`targets`) is moved to the GPU (`inputs.to(device)` and `targets.to(device)`).\n",
    "\n",
    "3. **Forward Pass**:\n",
    "   - The model performs a forward pass on the inputs to compute predictions (`outputs = net(inputs)`).\n",
    "\n",
    "4. **Loss and Accuracy Calculation**:\n",
    "   - The loss for the batch is calculated using the `criterion` (cross-entropy loss), and the total loss is accumulated.\n",
    "   - The predicted class labels are computed by finding the index of the maximum value in the output logits (`_, predicted = outputs.max(1)`).\n",
    "   - The number of correct predictions is counted and accumulated.\n",
    "\n",
    "5. **Epoch Summary**:\n",
    "   - After processing all batches in the test set, the function prints the overall test accuracy and average loss.\n",
    "\n",
    "6. **Saving the Model**:\n",
    "   - The model's state dictionary (which contains the learned weights) is saved to a file in a folder named `checkpoint`. This allows you to load the model later for inference or further training.\n",
    "\n",
    "This function allows you to evaluate the model's performance on the test set and save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    print('\\n[ Test epoch: %d ]' % epoch)\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Loop through the test data\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move to GPU\n",
    "        total += targets.size(0)  # Update total number of samples processed\n",
    "\n",
    "        outputs = net(inputs)  # Perform a forward pass through the model\n",
    "        loss += criterion(outputs, targets).item()  # Accumulate loss\n",
    "\n",
    "        _, predicted = outputs.max(1)  # Get the predicted labels (index of max logit)\n",
    "        correct += predicted.eq(targets).sum().item()  # Count correct predictions\n",
    "\n",
    "    # Print the final test results for the epoch\n",
    "    print('\\nTest accuracy:', 100. * correct / total)\n",
    "    print('Test average loss:', loss / total)\n",
    "\n",
    "    # Save the model's state\n",
    "    state = {\n",
    "        'net': net.state_dict()\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')  # Create 'checkpoint' directory if it doesn't exist\n",
    "    torch.save(state, './checkpoint/' + file_name)  # Save the model\n",
    "    print('Model Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Adjustment Function\n",
    "\n",
    "The `adjust_learning_rate()` function dynamically adjusts the learning rate during training based on the epoch number. This is a common strategy to improve training stability and convergence as the model approaches an optimal solution.\n",
    "\n",
    "1. **Initial Learning Rate**: \n",
    "   - The function starts with an initial learning rate defined by `learning_rate`.\n",
    "\n",
    "2. **Learning Rate Scheduling**:\n",
    "   - If the epoch number reaches 100, the learning rate is reduced by a factor of 10. This is a typical approach to decrease the learning rate after a certain number of epochs, allowing the model to refine its weights more carefully.\n",
    "   - If the epoch number reaches 150, the learning rate is again reduced by another factor of 10. This helps the model converge more precisely as training progresses.\n",
    "\n",
    "3. **Update the Optimizer**:\n",
    "   - The learning rate is updated for all parameter groups in the optimizer using `optimizer.param_groups`. This ensures that the optimizer uses the adjusted learning rate during the next step.\n",
    "\n",
    "This strategy of reducing the learning rate over time is often called **Step Decay** and can help the model reach a better minimum in the loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate  # Start with the initial learning rate\n",
    "    if epoch >= 100:    # After 100 epochs, reduce learning rate by 10x\n",
    "        lr /= 10\n",
    "    if epoch >= 150:    # After 150 epochs, reduce learning rate by another 10x\n",
    "        lr /= 10\n",
    "    \n",
    "    # Update the learning rate for all parameter groups in the optimizer\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing the Model for 20 Epochs\n",
    "\n",
    "In this section, we run the training and testing loops for 20 epochs. During each epoch:\n",
    "\n",
    "1. **Learning Rate Adjustment**:  \n",
    "   - The learning rate is adjusted according to the epoch number using the `adjust_learning_rate()` function. This ensures that the learning rate decreases at specific milestones (100 and 150 epochs) to allow for more fine-grained updates towards the end of the training.\n",
    "\n",
    "2. **Training Loop**:  \n",
    "   - The `train()` function is called to train the model on the training dataset for one epoch. This function computes the loss, updates the model weights, and tracks training accuracy.\n",
    "\n",
    "3. **Testing Loop**:  \n",
    "   - The `test()` function is called to evaluate the model on the test dataset after each epoch. This function computes the loss and accuracy on the test set and saves the model after each test.\n",
    "\n",
    "This loop will train and evaluate the model for a total of 20 epochs, helping us monitor the performance of the model over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Train epoch: 0 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.078125\n",
      "Current benign train loss: 2.51173996925354\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.2890625\n",
      "Current benign train loss: 1.8501043319702148\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.28125\n",
      "Current benign train loss: 1.7279845476150513\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.3828125\n",
      "Current benign train loss: 1.591795802116394\n",
      "\n",
      "Total benign train accuracy: 32.708\n",
      "Total benign train loss: 721.3459944725037\n",
      "\n",
      "[ Test epoch: 0 ]\n",
      "\n",
      "Test accuracy: 39.27\n",
      "Test average loss: 0.017211592411994932\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 1 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.4375\n",
      "Current benign train loss: 1.477840542793274\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.46875\n",
      "Current benign train loss: 1.5304198265075684\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.5390625\n",
      "Current benign train loss: 1.3074949979782104\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.6171875\n",
      "Current benign train loss: 1.1139072179794312\n",
      "\n",
      "Total benign train accuracy: 48.776\n",
      "Total benign train loss: 548.6893157958984\n",
      "\n",
      "[ Test epoch: 1 ]\n",
      "\n",
      "Test accuracy: 24.87\n",
      "Test average loss: 0.034098321795463564\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 2 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.515625\n",
      "Current benign train loss: 1.3008971214294434\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.5703125\n",
      "Current benign train loss: 1.1169010400772095\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.6640625\n",
      "Current benign train loss: 1.014182448387146\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.5703125\n",
      "Current benign train loss: 1.194717288017273\n",
      "\n",
      "Total benign train accuracy: 59.258\n",
      "Total benign train loss: 442.59407287836075\n",
      "\n",
      "[ Test epoch: 2 ]\n",
      "\n",
      "Test accuracy: 49.28\n",
      "Test average loss: 0.017492403614521028\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 3 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.6328125\n",
      "Current benign train loss: 1.081469178199768\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.6171875\n",
      "Current benign train loss: 1.0151745080947876\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.703125\n",
      "Current benign train loss: 0.9723840951919556\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 0.7455058097839355\n",
      "\n",
      "Total benign train accuracy: 66.05\n",
      "Total benign train loss: 372.06062763929367\n",
      "\n",
      "[ Test epoch: 3 ]\n",
      "\n",
      "Test accuracy: 67.09\n",
      "Test average loss: 0.009301169955730439\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 4 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 0.7145771384239197\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.7265625\n",
      "Current benign train loss: 0.8533129096031189\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.7265625\n",
      "Current benign train loss: 0.8057961463928223\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.7421875\n",
      "Current benign train loss: 0.7204079031944275\n",
      "\n",
      "Total benign train accuracy: 71.444\n",
      "Total benign train loss: 318.21986496448517\n",
      "\n",
      "[ Test epoch: 4 ]\n",
      "\n",
      "Test accuracy: 67.16\n",
      "Test average loss: 0.009690751457214356\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 5 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7265625\n",
      "Current benign train loss: 0.6724656224250793\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.71875\n",
      "Current benign train loss: 0.7997563481330872\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.6953125\n",
      "Current benign train loss: 0.8825278282165527\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.7734375\n",
      "Current benign train loss: 0.667961061000824\n",
      "\n",
      "Total benign train accuracy: 75.34\n",
      "Total benign train loss: 274.2477470934391\n",
      "\n",
      "[ Test epoch: 5 ]\n",
      "\n",
      "Test accuracy: 65.89\n",
      "Test average loss: 0.010570636957883834\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 6 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7421875\n",
      "Current benign train loss: 0.660001814365387\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.875\n",
      "Current benign train loss: 0.46672311425209045\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.75\n",
      "Current benign train loss: 0.743647575378418\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8125\n",
      "Current benign train loss: 0.7099748849868774\n",
      "\n",
      "Total benign train accuracy: 78.918\n",
      "Total benign train loss: 238.50771048665047\n",
      "\n",
      "[ Test epoch: 6 ]\n",
      "\n",
      "Test accuracy: 74.35\n",
      "Test average loss: 0.007527841520309448\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 7 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.5156634449958801\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.78125\n",
      "Current benign train loss: 0.5525364875793457\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.4476131498813629\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.5251035690307617\n",
      "\n",
      "Total benign train accuracy: 81.12\n",
      "Total benign train loss: 212.5364001095295\n",
      "\n",
      "[ Test epoch: 7 ]\n",
      "\n",
      "Test accuracy: 76.04\n",
      "Test average loss: 0.006901738259196282\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 8 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.5099765062332153\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.4415762424468994\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8125\n",
      "Current benign train loss: 0.5626490116119385\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.4477654993534088\n",
      "\n",
      "Total benign train accuracy: 82.532\n",
      "Total benign train loss: 196.4094690978527\n",
      "\n",
      "[ Test epoch: 8 ]\n",
      "\n",
      "Test accuracy: 73.43\n",
      "Test average loss: 0.008464233094453811\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 9 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.5528857111930847\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8125\n",
      "Current benign train loss: 0.4963793456554413\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.5046248435974121\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.41328054666519165\n",
      "\n",
      "Total benign train accuracy: 84.226\n",
      "Total benign train loss: 179.15282505750656\n",
      "\n",
      "[ Test epoch: 9 ]\n",
      "\n",
      "Test accuracy: 81.17\n",
      "Test average loss: 0.0055383963882923125\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 10 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.3761236071586609\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.796875\n",
      "Current benign train loss: 0.4962954521179199\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.3293607532978058\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.3995896577835083\n",
      "\n",
      "Total benign train accuracy: 85.344\n",
      "Total benign train loss: 165.76181806623936\n",
      "\n",
      "[ Test epoch: 10 ]\n",
      "\n",
      "Test accuracy: 82.32\n",
      "Test average loss: 0.005410649999976158\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 11 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.4218693673610687\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.875\n",
      "Current benign train loss: 0.34572726488113403\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.4777469336986542\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.4664781391620636\n",
      "\n",
      "Total benign train accuracy: 86.092\n",
      "Total benign train loss: 156.57347647845745\n",
      "\n",
      "[ Test epoch: 11 ]\n",
      "\n",
      "Test accuracy: 81.55\n",
      "Test average loss: 0.005690200120210647\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 12 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.3484073579311371\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.9140625\n",
      "Current benign train loss: 0.2718799412250519\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.2904810309410095\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.37857764959335327\n",
      "\n",
      "Total benign train accuracy: 87.09\n",
      "Total benign train loss: 146.0629225820303\n",
      "\n",
      "[ Test epoch: 12 ]\n",
      "\n",
      "Test accuracy: 79.37\n",
      "Test average loss: 0.0064313464283943175\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 13 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.3655349314212799\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.3360236585140228\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.4578876793384552\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.33229899406433105\n",
      "\n",
      "Total benign train accuracy: 87.858\n",
      "Total benign train loss: 138.57698510587215\n",
      "\n",
      "[ Test epoch: 13 ]\n",
      "\n",
      "Test accuracy: 81.3\n",
      "Test average loss: 0.005673661431670189\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 14 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.9375\n",
      "Current benign train loss: 0.2417423278093338\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.478037565946579\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.43083903193473816\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.875\n",
      "Current benign train loss: 0.3525387644767761\n",
      "\n",
      "Total benign train accuracy: 88.384\n",
      "Total benign train loss: 131.79188556969166\n",
      "\n",
      "[ Test epoch: 14 ]\n",
      "\n",
      "Test accuracy: 84.09\n",
      "Test average loss: 0.0046257209718227384\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 15 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.9296875\n",
      "Current benign train loss: 0.24088379740715027\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.921875\n",
      "Current benign train loss: 0.25472593307495117\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.32168811559677124\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.27793124318122864\n",
      "\n",
      "Total benign train accuracy: 88.788\n",
      "Total benign train loss: 126.39477574825287\n",
      "\n",
      "[ Test epoch: 15 ]\n",
      "\n",
      "Test accuracy: 79.11\n",
      "Test average loss: 0.006713709053397179\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 16 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.20594516396522522\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.921875\n",
      "Current benign train loss: 0.24691961705684662\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.953125\n",
      "Current benign train loss: 0.23713195323944092\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.29766085743904114\n",
      "\n",
      "Total benign train accuracy: 89.11\n",
      "Total benign train loss: 123.9425808340311\n",
      "\n",
      "[ Test epoch: 16 ]\n",
      "\n",
      "Test accuracy: 86.03\n",
      "Test average loss: 0.004324780470132828\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 17 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.29234567284584045\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.30523401498794556\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.5093197822570801\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.42843496799468994\n",
      "\n",
      "Total benign train accuracy: 89.45\n",
      "Total benign train loss: 119.14640711247921\n",
      "\n",
      "[ Test epoch: 17 ]\n",
      "\n",
      "Test accuracy: 82.63\n",
      "Test average loss: 0.0055897444546222684\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 18 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.26348358392715454\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.28748270869255066\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.45244529843330383\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.4294397234916687\n",
      "\n",
      "Total benign train accuracy: 89.816\n",
      "Total benign train loss: 114.58751344680786\n",
      "\n",
      "[ Test epoch: 18 ]\n",
      "\n",
      "Test accuracy: 84.44\n",
      "Test average loss: 0.005033125424385071\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 19 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.40161067247390747\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.4063502848148346\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.322211354970932\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.9296875\n",
      "Current benign train loss: 0.2349739521741867\n",
      "\n",
      "Total benign train accuracy: 90.24\n",
      "Total benign train loss: 110.65296998620033\n",
      "\n",
      "[ Test epoch: 19 ]\n",
      "\n",
      "Test accuracy: 86.95\n",
      "Test average loss: 0.00400381117016077\n",
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "# Train and test the model for 20 epochs\n",
    "for epoch in range(0, 20):\n",
    "    # Adjust the learning rate based on the epoch number\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    \n",
    "    # Train the model for the current epoch\n",
    "    train(epoch)\n",
    "    \n",
    "    # Test the model after the current epoch\n",
    "    test(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cae4ca236d0e60a4325c83670b9a02628cb23562543787e656aa9886bd07c247"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
